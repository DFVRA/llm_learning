### dropout 为什么有效
在训练模型的时候，如果数据量比较少，为了降低损失，往往会增大 epoch，这种做法会使得模型在训练集上面表现不错，但在验证集上表现却差强人意。这种现象可以理解为模型记住了训练数据，而不是真正学习到了数据特征。这种现象被称为过拟合。
* Dropout在一定程度上可以缓解过拟合的现象。
Dropout的定义是：在训练模型的时候，模型中的神经元会进行随机失活，每个神经元失活的概率记为 p，因此每次训练的时候，模型更新的神经元并不固定，模型的效果不会依赖于某几个特定的神经元，提高了模型的鲁棒性。
### 计算注意力分数的时候为什么要加缩放因子 根号 d_k
Attention is All You Need 原始论文中的解释是：当向量维度变大的时候，softmax函数会造成梯度消失的问题，所以设置了一个 softmax 的缩放因子来缓解梯度消失的问题。这里面有两个问题：
1.为什么向量维度变大的时候，softmax会造成梯度消失的问题。
2.为什么使用根号 d_k 作为缩放因子，有更好的值吗
* d_k 变大，方差会变大：Q K 点积的方差 = d_k

假设 Q K 的长度为 d_k，服从独立同分布，均值为0， 方差为1的正态分布。 var[Q*K.T] = sum(var(Q_i * K_i)) = sum(var(Q_i)) * sum(var(K_i)) = d_k
所以当 d_k 变大的时候，点积的方差也会变大
* 方差变大，向量元素之间的差值会变大
* softmax 退化为 argmax

当某个注意力分数很大的时候，softmax之后对应的值会非常接近1，而其他位置则非常接近0 类似于argmax的效果 
* softmax 什么情况下梯度会消失
softmax 的导数可以表示为一个雅可比矩阵，对角线元素为 y_i * (1 - y_i), 对角线以外，第 i 行，第 j 列的元素表示 y_i 对 z_j 的偏导，值为 -y_i * y_j
如果 softmax 退化为 argmax, 大多数 y_i 会变成零，这个雅可比矩阵会变成一个全零矩阵，导致了梯度消失的问题。
* 为什么选择 根号d_k 作为缩放因子
经过上述推导可以发现，导致梯度消失的本质原因是 Q 与 K 做点积的时候方差变成了 d_k， 加入缩放因子可以把方差变为 1
