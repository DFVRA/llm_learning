Tokenizer的作用是**将自然语言转变为机器可以理解的数字表示**。自然语言经过 Tokenizer 的处理后，就可以被机器进一步学习。
主流的 Tokenizer 有如下几种。
### 古典分词法：基于 word 和 character 的分词方法
#### 一 基于 word 的分词方法
基于 word 的分词方法比较容易实现，例如英文，只需要将句子按照空格切分即可实现。这种方法看着确实简单一些，优点是：
* token 是一个完整的单词，语义信息是完整的。

但是缺点也比较多。
* 这种分词方法会得到非常大的词汇表，所有单词都会被放入词汇表中，这导致不同形态的单词会被当作不同的词来处理，**词汇表太大了，而且明显是冗余的**。
* 训练时，由于显存限制，很难将完整的词汇表加载，例如选取词频比较高的一部分词表，这导致词频比较低的单词无法得到训练，只能当作[UNK]来处理。
#### 二 基于 character 的分词
这种方法可以很好的避免词汇表过大的问题，如果语料均为英文，词表只需要包括26英文字母以及符号即可。但缺点也比较明显。
* 如果现在有一个句子基于 character 进行分词，**每个字母均需要被单独表示**，这导致模型处理起来比较吃力。举个例子：How are you? 仅仅三个单词，字母，空格以及标点符号一共需要12个数字表示。如果句子再长一点，序列表示会非常长。模型的计算时间复杂度是 o(n^2) n为序列长度。
* 词汇表里面的 token 只有字母和标点符号，语义信息量较低。

#### 总结一下上述两种分词方法：
* 基于 word 的分词方法可以包含完整单词的语义信息，但词表明显冗余，模型无法处理没见过的单词。
* 基于 character 的分词 可以避免 OOV（未登记词）问题，但 token 语义信息较低，序列表示过长。
* 综合上述两种分词方法的优缺点，我们需要的是一种 token 语义信息较高，不冗余，序列表示不会太长，并可以较好的处理 OOV问题的分词方法。基于子词的分词方法便满足我们的要求。
### 基于子词的分词方法
高频词依旧为完整的单词，低频词被切分为有意义的子词。
* 完整的单词和子词均具备语义。
* 序列表示不会太长，高频词只需要一个数字表示，低频词可以表示为不同子词的组合。
* 词表有限，例如 不同形态单词可以表示为 基本单词+形态后缀，极大程度上降低了词表的大小
* 一定程度上解决 OOV 问题，将未登记词汇分解为子词表示来进一步处理。

三种常见的基于子词的分词方法：Byte Pair Encoding(BPE), WorldPiece 和 Unigram Language Model
#### Byte Pair Encoding(BPE)
这种分词算法比较流行，BERT GPT LlaMa Qwen 等模型均在使用。
##### BPE算法过程
* 预处理
    * 语料标准化（小写，Unicode规范化）
    * 在每个单词尾部加入后缀 \</w> 用于标注词尾（区分跨单词边界，例如 st 在 start\</w> 和 fast\</w>不同），统计每个单词出现的频率。例如 low 的频率为5，则将其改写为 “low\</w>”: 5
* 初始化词汇表
    * 将语料库中的所有单词拆分为单个字符，用单个字符建立为最初的字典，并统计每个字符的频率

* 迭代合并
    * 统计相邻符号对频率
    * 合并最高频对
        * 更新词汇表：新增最高频对，移除单个独立计数
        * 更新语料
    * 重复直到达到目标词表大小
##### WorldPiece 算法
在 BPE 算法中，每次合并的是出现频率最高的 pair，WorldPiece 合并标准是计算得分：score = (freq_of_pair) / (freq_of_first_element * freq_of_secong_element) 优先合并得分比较小的 pair
因为相比于 BPE WorldPiece 多了分母，如果 pair 中的元素只在 pair中出现，那么分数将为 1/n ，但是如果在 fair 之外的地方也出现了该 pair 里面的元素，那么分母会大于分子，得分会小于 1/n ，因此 该算法会优先合并单独出现的 pair.得分越高，说明 pair的出现更有意义，合并对文本表示更有意义。
##### Unigram 算法
与 BPE 和 WordPiece 不同，Unigram 从一个大的词汇表开始逐步删除词汇，直到达到目标词汇的大小。在训练的每一步，会在当前词汇表下语料库的损失，然后对于词汇表中的每个符号，计算如果删除该符号，损失会增加多少，删除带来最低损失的符号（不会删除单个字符，确保所有词汇都可以被切分）但是这个过程比较消耗计算资源，一般是删除损失的 top-p.




