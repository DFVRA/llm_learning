如果在计算 attention 的时候没有加位置编码，会发现输入和输出会保持某种一致性，也就是说输入的 token 序列发生变化的时候，输出的具体内容不会发生变化，只是输出的顺序发生了变化。
这种情况下，模型无法学习依赖 token 序列的语义。输入的 token 序列发生变化，输出也仅仅是位置上发生了变化。加入位置编码后，每个位置的 token 被注入唯一的位置信息，
使得模型可以感知到 token 序列信息。

### 位置编码
#### 正弦余弦位置编码
PE(pos, 2i) = sin(pos / 10000^(2i / d_model))

PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))

pos: 1, 2, 3, ..., seq_len

0 <= i < d_model / 2 